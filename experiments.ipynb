{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install torch\n",
    "# %pip install emoji==0.6.0\n",
    "# %pip install TweetNormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk import WordNetLemmatizer\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import json\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "nltk.download('punkt')\n",
    "alt.data_transformers.disable_max_rows()\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\Users\\Administrator\\Downloads\\parler-hate-speech-main\\parler-hate-speech-main\\parler_annotated_data.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #split too long posts\n",
    "df.text = df.text.apply(lambda x: [x for x in split_rows(x)])\n",
    "df = df.explode(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = Model(TokensClassTasks.NER)\n",
    "#inference\n",
    "df['ner'] = df.text.apply(ner_model.run_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag_model = Model(TokensClassTasks.POS_TAGGING)\n",
    "#inference\n",
    "df['pos_tag'] = df.text.apply(pos_tag_model.run_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"ner_pos_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov  4 2022, 13:42:51) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bf672514722649c087606e81e119605c1cc4764f9792a89fd7dcda5a6b35db6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
